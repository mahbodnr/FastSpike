{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIN2Net_MI-EEG-Classification_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahbodnr/FastSpike/blob/main/MLP_MIN2Netipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[<img src=\"https://min2net.github.io/assets/images/min2net-logo.png\" width=\"30%\" height=\"30%\">](https://min2net.github.io)\n",
        "\n",
        "### End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1IE5J0Yn10ZIhWjSatQn_QWJWZblr6tZy?usp=sharing)\n",
        "[![Pypi Downloads](https://img.shields.io/pypi/v/min2net?color=green&logo=pypi&logoColor=white)](https://pypi.org/project/min2net)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.1109%2FTBME.2021.3137184-blue)](https://ieeexplore.ieee.org/document/9658165)\n",
        "\n",
        "Python API and the novel algorithm for motor imagery EEG recognition named MIN2Net. The API benefits BCI researchers ranging from beginners to experts. We demonstrate the examples in using the API for loading benchmark datasets, preprocessing, training, and validation of SOTA models, including MIN2Net. In summary, the API allows the researchers to construct the pipeline for benchmarking the newly proposed models and very recently developed SOTA models.\n"
      ],
      "metadata": {
        "id": "Hg_tclhtNgx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Website:** [https://min2net.github.io](https://min2net.github.io)\n",
        "- **Documentation:** [https://min2net.github.io](https://min2net.github.io)\n",
        "- **Source code:** [https://github.com/IoBT-VISTEC/MIN2Net](https://github.com/IoBT-VISTEC/MIN2Net)\n",
        "- **Bug reports:** [https://github.com/IoBT-VISTEC/MIN2Net/issues](https://github.com/IoBT-VISTEC/MIN2Net/issues)"
      ],
      "metadata": {
        "id": "dji6MI3GNp4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "TyuGaaagIg8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install min2net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0sFW9BeRY_C",
        "outputId": "f9821543-eb43-44fe-f106-e182d2fa024b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: min2net in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.37.0 in /usr/local/lib/python3.7/dist-packages (from min2net) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.7/dist-packages (from min2net) (57.4.0)\n",
            "Requirement already satisfied: wget>=3.2 in /usr/local/lib/python3.7/dist-packages (from min2net) (3.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from min2net) (1.0.2)\n",
            "Requirement already satisfied: tensorflow-addons==0.9.1 in /usr/local/lib/python3.7/dist-packages (from min2net) (0.9.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.9.1->min2net) (2.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->min2net) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess dada"
      ],
      "metadata": {
        "id": "OpUnRxfNWc9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import min2net\n",
        "import min2net.preprocessing as prep\n",
        "\n",
        "\n",
        "min2net.utils.load_raw('BCIC2a')\n",
        "prep.BCIC2a.time_domain.subject_dependent_setting(k_folds=5,\n",
        "                                                 pick_smp_freq=100, \n",
        "                                                 bands=[8, 30], \n",
        "                                                 order=5, \n",
        "                                                 save_path='datasets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHtg7Tu5WdVB",
        "outputId": "efce44c5-8c4c-4118-ba4a-3bd4f81977a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.1.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creare DataLoader"
      ],
      "metadata": {
        "id": "3IhZkgNJW_cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from min2net.utils import DataLoader\n",
        "\n",
        "loader = DataLoader(dataset='BCIC2a', \n",
        "                    train_type='subject_dependent', \n",
        "                    subject=1, \n",
        "                    data_format='NTCD', # for MIN2Net\n",
        "                    data_type='time_domain', \n",
        "                    dataset_path='datasets')"
      ],
      "metadata": {
        "id": "uZhGpGOaW_9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MIN2Net model example"
      ],
      "metadata": {
        "id": "K-Kn7p1VVXsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "X_train, y_train = loader.load_train_set(fold=1)\n",
        "X_val, y_val = loader.load_val_set(fold=1)\n",
        "X_test, y_test = loader.load_test_set(fold=1)\n",
        "\n",
        "\n",
        "from min2net.model import MIN2Net\n",
        "model = MIN2Net(input_shape=(1, 400, 20), num_class=2, monitor='val_loss', shuffle=True)\n",
        "model.fit(X_train, y_train, X_val, y_val)\n",
        "Y, evaluation = model.predict(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWFKPEHRlcN",
        "outputId": "ca989c47-b1d0-4afc-c51d-439f8408701f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change data_format to 'NTCD', new dimention is (115, 1, 400, 20)\n",
            "change data_format to 'NTCD', new dimention is (29, 1, 400, 20)\n",
            "change data_format to 'NTCD', new dimention is (144, 1, 400, 20)\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1, 400, 20)]      0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 1, 400, 20)        25620     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1, 400, 20)       80        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 1, 100, 20)       0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 1, 100, 10)        6410      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1, 100, 10)       40        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 1, 25, 10)        0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 250)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                5020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,170\n",
            "Trainable params: 37,110\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_input (InputLayer)  [(None, 20)]              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 250)               5250      \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 1, 25, 10)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 1, 100, 10)       6410      \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 1, 400, 20)       6420      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,080\n",
            "Trainable params: 18,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"MIN2Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1, 400, 20)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, 20)           37170       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, 1, 400, 20)   18080       ['encoder[0][0]']                \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 2)            42          ['encoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55,292\n",
            "Trainable params: 55,232\n",
            "Non-trainable params: 60\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 23.1932 - decoder_loss: 21.6366 - encoder_loss: 0.8176 - classifier_loss: 0.7390 - decoder_accuracy: 0.0535 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.5391\n",
            "Epoch 00001: val_loss improved from inf to 22.13223, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 5s 915ms/step - loss: 23.1932 - decoder_loss: 21.6366 - encoder_loss: 0.8176 - classifier_loss: 0.7390 - decoder_accuracy: 0.0535 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.5391 - val_loss: 22.1322 - val_decoder_loss: 20.6856 - val_encoder_loss: 0.6740 - val_classifier_loss: 0.7727 - val_decoder_accuracy: 0.0535 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 23.5353 - decoder_loss: 21.6321 - encoder_loss: 1.2140 - classifier_loss: 0.6891 - decoder_accuracy: 0.0530 - encoder_accuracy: 0.0261 - classifier_accuracy: 0.5391\n",
            "Epoch 00002: val_loss did not improve from 22.13223\n",
            "2/2 [==============================] - 1s 224ms/step - loss: 23.5353 - decoder_loss: 21.6321 - encoder_loss: 1.2140 - classifier_loss: 0.6891 - decoder_accuracy: 0.0530 - encoder_accuracy: 0.0261 - classifier_accuracy: 0.5391 - val_loss: 22.1929 - val_decoder_loss: 20.6796 - val_encoder_loss: 0.7684 - val_classifier_loss: 0.7449 - val_decoder_accuracy: 0.0478 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 23.1882 - decoder_loss: 21.6085 - encoder_loss: 0.8996 - classifier_loss: 0.6801 - decoder_accuracy: 0.0481 - encoder_accuracy: 0.0174 - classifier_accuracy: 0.5391\n",
            "Epoch 00003: val_loss did not improve from 22.13223\n",
            "2/2 [==============================] - 1s 234ms/step - loss: 23.1882 - decoder_loss: 21.6085 - encoder_loss: 0.8996 - classifier_loss: 0.6801 - decoder_accuracy: 0.0481 - encoder_accuracy: 0.0174 - classifier_accuracy: 0.5391 - val_loss: 22.1855 - val_decoder_loss: 20.6731 - val_encoder_loss: 0.7943 - val_classifier_loss: 0.7180 - val_decoder_accuracy: 0.0437 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.3793 - lr: 0.0100\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 23.0357 - decoder_loss: 21.5799 - encoder_loss: 0.7847 - classifier_loss: 0.6711 - decoder_accuracy: 0.0461 - encoder_accuracy: 0.0174 - classifier_accuracy: 0.6435\n",
            "Epoch 00004: val_loss did not improve from 22.13223\n",
            "2/2 [==============================] - 1s 244ms/step - loss: 23.0357 - decoder_loss: 21.5799 - encoder_loss: 0.7847 - classifier_loss: 0.6711 - decoder_accuracy: 0.0461 - encoder_accuracy: 0.0174 - classifier_accuracy: 0.6435 - val_loss: 22.2015 - val_decoder_loss: 20.7020 - val_encoder_loss: 0.7926 - val_classifier_loss: 0.7069 - val_decoder_accuracy: 0.0441 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4138 - lr: 0.0100\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 23.3198 - decoder_loss: 21.5311 - encoder_loss: 1.1302 - classifier_loss: 0.6584 - decoder_accuracy: 0.0495 - encoder_accuracy: 0.0261 - classifier_accuracy: 0.6783\n",
            "Epoch 00005: val_loss improved from 22.13223 to 22.07466, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 23.3198 - decoder_loss: 21.5311 - encoder_loss: 1.1302 - classifier_loss: 0.6584 - decoder_accuracy: 0.0495 - encoder_accuracy: 0.0261 - classifier_accuracy: 0.6783 - val_loss: 22.0747 - val_decoder_loss: 20.6867 - val_encoder_loss: 0.6747 - val_classifier_loss: 0.7133 - val_decoder_accuracy: 0.0447 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0100\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 22.9112 - decoder_loss: 21.4078 - encoder_loss: 0.8589 - classifier_loss: 0.6446 - decoder_accuracy: 0.0524 - encoder_accuracy: 0.0087 - classifier_accuracy: 0.6696\n",
            "Epoch 00006: val_loss did not improve from 22.07466\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 22.9112 - decoder_loss: 21.4078 - encoder_loss: 0.8589 - classifier_loss: 0.6446 - decoder_accuracy: 0.0524 - encoder_accuracy: 0.0087 - classifier_accuracy: 0.6696 - val_loss: 22.2267 - val_decoder_loss: 20.6518 - val_encoder_loss: 0.8532 - val_classifier_loss: 0.7218 - val_decoder_accuracy: 0.0513 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0100\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 22.8222 - decoder_loss: 21.2922 - encoder_loss: 0.9025 - classifier_loss: 0.6275 - decoder_accuracy: 0.0586 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.6783\n",
            "Epoch 00007: val_loss did not improve from 22.07466\n",
            "2/2 [==============================] - 1s 249ms/step - loss: 22.8222 - decoder_loss: 21.2922 - encoder_loss: 0.9025 - classifier_loss: 0.6275 - decoder_accuracy: 0.0586 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.6783 - val_loss: 22.2158 - val_decoder_loss: 20.5861 - val_encoder_loss: 0.8954 - val_classifier_loss: 0.7343 - val_decoder_accuracy: 0.0584 - val_encoder_accuracy: 0.0690 - val_classifier_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 22.5352 - decoder_loss: 21.1442 - encoder_loss: 0.7809 - classifier_loss: 0.6101 - decoder_accuracy: 0.0632 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.6957\n",
            "Epoch 00008: val_loss did not improve from 22.07466\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 22.5352 - decoder_loss: 21.1442 - encoder_loss: 0.7809 - classifier_loss: 0.6101 - decoder_accuracy: 0.0632 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.6957 - val_loss: 22.2081 - val_decoder_loss: 20.4819 - val_encoder_loss: 0.9654 - val_classifier_loss: 0.7608 - val_decoder_accuracy: 0.0616 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4483 - lr: 0.0100\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 22.4490 - decoder_loss: 20.9723 - encoder_loss: 0.8849 - classifier_loss: 0.5918 - decoder_accuracy: 0.0703 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.7130\n",
            "Epoch 00009: val_loss did not improve from 22.07466\n",
            "2/2 [==============================] - 1s 228ms/step - loss: 22.4490 - decoder_loss: 20.9723 - encoder_loss: 0.8849 - classifier_loss: 0.5918 - decoder_accuracy: 0.0703 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.7130 - val_loss: 22.3061 - val_decoder_loss: 20.3759 - val_encoder_loss: 1.1475 - val_classifier_loss: 0.7827 - val_decoder_accuracy: 0.0667 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 22.1056 - decoder_loss: 20.7802 - encoder_loss: 0.7558 - classifier_loss: 0.5696 - decoder_accuracy: 0.0732 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.7478\n",
            "Epoch 00010: val_loss did not improve from 22.07466\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 22.1056 - decoder_loss: 20.7802 - encoder_loss: 0.7558 - classifier_loss: 0.5696 - decoder_accuracy: 0.0732 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.7478 - val_loss: 22.1760 - val_decoder_loss: 20.2651 - val_encoder_loss: 1.1189 - val_classifier_loss: 0.7920 - val_decoder_accuracy: 0.0672 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0100\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 22.0058 - decoder_loss: 20.6372 - encoder_loss: 0.8212 - classifier_loss: 0.5474 - decoder_accuracy: 0.0750 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.7826\n",
            "Epoch 00011: val_loss did not improve from 22.07466\n",
            "2/2 [==============================] - 1s 312ms/step - loss: 22.0058 - decoder_loss: 20.6372 - encoder_loss: 0.8212 - classifier_loss: 0.5474 - decoder_accuracy: 0.0750 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.7826 - val_loss: 22.0983 - val_decoder_loss: 20.2426 - val_encoder_loss: 1.0631 - val_classifier_loss: 0.7926 - val_decoder_accuracy: 0.0696 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4828 - lr: 0.0050\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.8617 - decoder_loss: 20.5460 - encoder_loss: 0.7849 - classifier_loss: 0.5308 - decoder_accuracy: 0.0768 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.8087\n",
            "Epoch 00012: val_loss improved from 22.07466 to 22.00770, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 472ms/step - loss: 21.8617 - decoder_loss: 20.5460 - encoder_loss: 0.7849 - classifier_loss: 0.5308 - decoder_accuracy: 0.0768 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.8087 - val_loss: 22.0077 - val_decoder_loss: 20.1801 - val_encoder_loss: 1.0417 - val_classifier_loss: 0.7860 - val_decoder_accuracy: 0.0705 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.8376 - decoder_loss: 20.4473 - encoder_loss: 0.8775 - classifier_loss: 0.5128 - decoder_accuracy: 0.0778 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.8174\n",
            "Epoch 00013: val_loss improved from 22.00770 to 21.93546, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 424ms/step - loss: 21.8376 - decoder_loss: 20.4473 - encoder_loss: 0.8775 - classifier_loss: 0.5128 - decoder_accuracy: 0.0778 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.8174 - val_loss: 21.9355 - val_decoder_loss: 20.1487 - val_encoder_loss: 1.0006 - val_classifier_loss: 0.7862 - val_decoder_accuracy: 0.0754 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4828 - lr: 0.0050\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.5583 - decoder_loss: 20.3683 - encoder_loss: 0.6982 - classifier_loss: 0.4918 - decoder_accuracy: 0.0805 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.8522\n",
            "Epoch 00014: val_loss improved from 21.93546 to 21.86940, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 310ms/step - loss: 21.5583 - decoder_loss: 20.3683 - encoder_loss: 0.6982 - classifier_loss: 0.4918 - decoder_accuracy: 0.0805 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.8522 - val_loss: 21.8694 - val_decoder_loss: 20.1072 - val_encoder_loss: 0.9602 - val_classifier_loss: 0.8020 - val_decoder_accuracy: 0.0776 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4483 - lr: 0.0050\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.4688 - decoder_loss: 20.3070 - encoder_loss: 0.6907 - classifier_loss: 0.4711 - decoder_accuracy: 0.0826 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.8957\n",
            "Epoch 00015: val_loss improved from 21.86940 to 21.85126, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 21.4688 - decoder_loss: 20.3070 - encoder_loss: 0.6907 - classifier_loss: 0.4711 - decoder_accuracy: 0.0826 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.8957 - val_loss: 21.8513 - val_decoder_loss: 20.0454 - val_encoder_loss: 0.9884 - val_classifier_loss: 0.8175 - val_decoder_accuracy: 0.0778 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4483 - lr: 0.0050\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.3132 - decoder_loss: 20.2417 - encoder_loss: 0.6225 - classifier_loss: 0.4490 - decoder_accuracy: 0.0838 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.9217\n",
            "Epoch 00016: val_loss improved from 21.85126 to 21.79949, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 385ms/step - loss: 21.3132 - decoder_loss: 20.2417 - encoder_loss: 0.6225 - classifier_loss: 0.4490 - decoder_accuracy: 0.0838 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.9217 - val_loss: 21.7995 - val_decoder_loss: 20.0259 - val_encoder_loss: 0.9610 - val_classifier_loss: 0.8126 - val_decoder_accuracy: 0.0772 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4138 - lr: 0.0050\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.3118 - decoder_loss: 20.1784 - encoder_loss: 0.7138 - classifier_loss: 0.4196 - decoder_accuracy: 0.0848 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.9391\n",
            "Epoch 00017: val_loss improved from 21.79949 to 21.73069, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 366ms/step - loss: 21.3118 - decoder_loss: 20.1784 - encoder_loss: 0.7138 - classifier_loss: 0.4196 - decoder_accuracy: 0.0848 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.9391 - val_loss: 21.7307 - val_decoder_loss: 20.0097 - val_encoder_loss: 0.9071 - val_classifier_loss: 0.8140 - val_decoder_accuracy: 0.0773 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4138 - lr: 0.0050\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 21.0583 - decoder_loss: 20.1295 - encoder_loss: 0.5387 - classifier_loss: 0.3900 - decoder_accuracy: 0.0851 - encoder_accuracy: 0.0261 - classifier_accuracy: 0.9478\n",
            "Epoch 00018: val_loss improved from 21.73069 to 21.63130, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 480ms/step - loss: 21.0583 - decoder_loss: 20.1295 - encoder_loss: 0.5387 - classifier_loss: 0.3900 - decoder_accuracy: 0.0851 - encoder_accuracy: 0.0261 - classifier_accuracy: 0.9478 - val_loss: 21.6313 - val_decoder_loss: 19.9978 - val_encoder_loss: 0.8423 - val_classifier_loss: 0.7911 - val_decoder_accuracy: 0.0814 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4138 - lr: 0.0050\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.8274 - decoder_loss: 20.1031 - encoder_loss: 0.3664 - classifier_loss: 0.3579 - decoder_accuracy: 0.0862 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.9565\n",
            "Epoch 00019: val_loss did not improve from 21.63130\n",
            "2/2 [==============================] - 1s 221ms/step - loss: 20.8274 - decoder_loss: 20.1031 - encoder_loss: 0.3664 - classifier_loss: 0.3579 - decoder_accuracy: 0.0862 - encoder_accuracy: 0.0435 - classifier_accuracy: 0.9565 - val_loss: 21.6486 - val_decoder_loss: 20.0250 - val_encoder_loss: 0.8554 - val_classifier_loss: 0.7681 - val_decoder_accuracy: 0.0797 - val_encoder_accuracy: 0.0690 - val_classifier_accuracy: 0.4483 - lr: 0.0050\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.7576 - decoder_loss: 20.0971 - encoder_loss: 0.3295 - classifier_loss: 0.3310 - decoder_accuracy: 0.0868 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.9565\n",
            "Epoch 00020: val_loss did not improve from 21.63130\n",
            "2/2 [==============================] - 1s 219ms/step - loss: 20.7576 - decoder_loss: 20.0971 - encoder_loss: 0.3295 - classifier_loss: 0.3310 - decoder_accuracy: 0.0868 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.9565 - val_loss: 21.6321 - val_decoder_loss: 20.0307 - val_encoder_loss: 0.8698 - val_classifier_loss: 0.7317 - val_decoder_accuracy: 0.0809 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4483 - lr: 0.0050\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.6104 - decoder_loss: 20.0905 - encoder_loss: 0.2215 - classifier_loss: 0.2985 - decoder_accuracy: 0.0868 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.9652\n",
            "Epoch 00021: val_loss improved from 21.63130 to 21.60132, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 20.6104 - decoder_loss: 20.0905 - encoder_loss: 0.2215 - classifier_loss: 0.2985 - decoder_accuracy: 0.0868 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.9652 - val_loss: 21.6013 - val_decoder_loss: 20.0294 - val_encoder_loss: 0.8678 - val_classifier_loss: 0.7042 - val_decoder_accuracy: 0.0797 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.4901 - decoder_loss: 20.0560 - encoder_loss: 0.1659 - classifier_loss: 0.2682 - decoder_accuracy: 0.0868 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.9826\n",
            "Epoch 00022: val_loss did not improve from 21.60132\n",
            "2/2 [==============================] - 1s 227ms/step - loss: 20.4901 - decoder_loss: 20.0560 - encoder_loss: 0.1659 - classifier_loss: 0.2682 - decoder_accuracy: 0.0868 - encoder_accuracy: 0.0609 - classifier_accuracy: 0.9826 - val_loss: 21.6282 - val_decoder_loss: 20.0358 - val_encoder_loss: 0.9048 - val_classifier_loss: 0.6875 - val_decoder_accuracy: 0.0787 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.4828 - lr: 0.0050\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.3662 - decoder_loss: 20.0131 - encoder_loss: 0.1089 - classifier_loss: 0.2443 - decoder_accuracy: 0.0857 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.9913\n",
            "Epoch 00023: val_loss did not improve from 21.60132\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 20.3662 - decoder_loss: 20.0131 - encoder_loss: 0.1089 - classifier_loss: 0.2443 - decoder_accuracy: 0.0857 - encoder_accuracy: 0.0522 - classifier_accuracy: 0.9913 - val_loss: 21.6201 - val_decoder_loss: 20.0211 - val_encoder_loss: 0.9188 - val_classifier_loss: 0.6803 - val_decoder_accuracy: 0.0796 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.2459 - decoder_loss: 19.9630 - encoder_loss: 0.0584 - classifier_loss: 0.2244 - decoder_accuracy: 0.0856 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.9913\n",
            "Epoch 00024: val_loss did not improve from 21.60132\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 20.2459 - decoder_loss: 19.9630 - encoder_loss: 0.0584 - classifier_loss: 0.2244 - decoder_accuracy: 0.0856 - encoder_accuracy: 0.0348 - classifier_accuracy: 0.9913 - val_loss: 21.6344 - val_decoder_loss: 20.0004 - val_encoder_loss: 0.9619 - val_classifier_loss: 0.6720 - val_decoder_accuracy: 0.0802 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.1578 - decoder_loss: 19.9111 - encoder_loss: 0.0374 - classifier_loss: 0.2093 - decoder_accuracy: 0.0867 - encoder_accuracy: 0.0435 - classifier_accuracy: 1.0000\n",
            "Epoch 00025: val_loss improved from 21.60132 to 21.57798, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 20.1578 - decoder_loss: 19.9111 - encoder_loss: 0.0374 - classifier_loss: 0.2093 - decoder_accuracy: 0.0867 - encoder_accuracy: 0.0435 - classifier_accuracy: 1.0000 - val_loss: 21.5780 - val_decoder_loss: 19.9667 - val_encoder_loss: 0.9410 - val_classifier_loss: 0.6704 - val_decoder_accuracy: 0.0786 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 20.0597 - decoder_loss: 19.8346 - encoder_loss: 0.0268 - classifier_loss: 0.1983 - decoder_accuracy: 0.0859 - encoder_accuracy: 0.0522 - classifier_accuracy: 1.0000\n",
            "Epoch 00026: val_loss improved from 21.57798 to 21.53734, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 381ms/step - loss: 20.0597 - decoder_loss: 19.8346 - encoder_loss: 0.0268 - classifier_loss: 0.1983 - decoder_accuracy: 0.0859 - encoder_accuracy: 0.0522 - classifier_accuracy: 1.0000 - val_loss: 21.5373 - val_decoder_loss: 19.8997 - val_encoder_loss: 0.9657 - val_classifier_loss: 0.6720 - val_decoder_accuracy: 0.0753 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.9513 - decoder_loss: 19.7479 - encoder_loss: 0.0143 - classifier_loss: 0.1890 - decoder_accuracy: 0.0869 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00027: val_loss improved from 21.53734 to 21.44569, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 397ms/step - loss: 19.9513 - decoder_loss: 19.7479 - encoder_loss: 0.0143 - classifier_loss: 0.1890 - decoder_accuracy: 0.0869 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.4457 - val_decoder_loss: 19.8408 - val_encoder_loss: 0.9336 - val_classifier_loss: 0.6713 - val_decoder_accuracy: 0.0776 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.8622 - decoder_loss: 19.6702 - encoder_loss: 0.0104 - classifier_loss: 0.1815 - decoder_accuracy: 0.0885 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00028: val_loss improved from 21.44569 to 21.42535, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 19.8622 - decoder_loss: 19.6702 - encoder_loss: 0.0104 - classifier_loss: 0.1815 - decoder_accuracy: 0.0885 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.4254 - val_decoder_loss: 19.7590 - val_encoder_loss: 0.9876 - val_classifier_loss: 0.6787 - val_decoder_accuracy: 0.0783 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.7829 - decoder_loss: 19.5998 - encoder_loss: 0.0078 - classifier_loss: 0.1754 - decoder_accuracy: 0.0878 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00029: val_loss improved from 21.42535 to 21.40168, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 410ms/step - loss: 19.7829 - decoder_loss: 19.5998 - encoder_loss: 0.0078 - classifier_loss: 0.1754 - decoder_accuracy: 0.0878 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.4017 - val_decoder_loss: 19.7136 - val_encoder_loss: 1.0020 - val_classifier_loss: 0.6861 - val_decoder_accuracy: 0.0785 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.6872 - decoder_loss: 19.5155 - encoder_loss: 0.0033 - classifier_loss: 0.1684 - decoder_accuracy: 0.0883 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00030: val_loss improved from 21.40168 to 21.34637, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 406ms/step - loss: 19.6872 - decoder_loss: 19.5155 - encoder_loss: 0.0033 - classifier_loss: 0.1684 - decoder_accuracy: 0.0883 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.3464 - val_decoder_loss: 19.6477 - val_encoder_loss: 1.0139 - val_classifier_loss: 0.6847 - val_decoder_accuracy: 0.0773 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.5939 - decoder_loss: 19.4284 - encoder_loss: 0.0021 - classifier_loss: 0.1634 - decoder_accuracy: 0.0886 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00031: val_loss improved from 21.34637 to 21.31802, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 323ms/step - loss: 19.5939 - decoder_loss: 19.4284 - encoder_loss: 0.0021 - classifier_loss: 0.1634 - decoder_accuracy: 0.0886 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.3180 - val_decoder_loss: 19.6135 - val_encoder_loss: 1.0173 - val_classifier_loss: 0.6873 - val_decoder_accuracy: 0.0776 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.5264 - decoder_loss: 19.3653 - encoder_loss: 0.0022 - classifier_loss: 0.1590 - decoder_accuracy: 0.0883 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00032: val_loss improved from 21.31802 to 21.30803, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 331ms/step - loss: 19.5264 - decoder_loss: 19.3653 - encoder_loss: 0.0022 - classifier_loss: 0.1590 - decoder_accuracy: 0.0883 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.3080 - val_decoder_loss: 19.5669 - val_encoder_loss: 1.0557 - val_classifier_loss: 0.6855 - val_decoder_accuracy: 0.0799 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.4704 - decoder_loss: 19.3137 - encoder_loss: 0.0026 - classifier_loss: 0.1541 - decoder_accuracy: 0.0881 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00033: val_loss improved from 21.30803 to 21.24314, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 393ms/step - loss: 19.4704 - decoder_loss: 19.3137 - encoder_loss: 0.0026 - classifier_loss: 0.1541 - decoder_accuracy: 0.0881 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.2431 - val_decoder_loss: 19.5483 - val_encoder_loss: 1.0122 - val_classifier_loss: 0.6826 - val_decoder_accuracy: 0.0816 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.4064 - decoder_loss: 19.2545 - encoder_loss: 0.0015 - classifier_loss: 0.1503 - decoder_accuracy: 0.0876 - encoder_accuracy: 0.0435 - classifier_accuracy: 1.0000\n",
            "Epoch 00034: val_loss improved from 21.24314 to 21.22871, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 405ms/step - loss: 19.4064 - decoder_loss: 19.2545 - encoder_loss: 0.0015 - classifier_loss: 0.1503 - decoder_accuracy: 0.0876 - encoder_accuracy: 0.0435 - classifier_accuracy: 1.0000 - val_loss: 21.2287 - val_decoder_loss: 19.5103 - val_encoder_loss: 1.0355 - val_classifier_loss: 0.6829 - val_decoder_accuracy: 0.0816 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.3451 - decoder_loss: 19.1941 - encoder_loss: 0.0010 - classifier_loss: 0.1500 - decoder_accuracy: 0.0895 - encoder_accuracy: 0.0435 - classifier_accuracy: 1.0000\n",
            "Epoch 00035: val_loss improved from 21.22871 to 21.18166, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 19.3451 - decoder_loss: 19.1941 - encoder_loss: 0.0010 - classifier_loss: 0.1500 - decoder_accuracy: 0.0895 - encoder_accuracy: 0.0435 - classifier_accuracy: 1.0000 - val_loss: 21.1817 - val_decoder_loss: 19.5072 - val_encoder_loss: 0.9953 - val_classifier_loss: 0.6792 - val_decoder_accuracy: 0.0827 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.2964 - decoder_loss: 19.1495 - encoder_loss: 0.0011 - classifier_loss: 0.1458 - decoder_accuracy: 0.0936 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00036: val_loss did not improve from 21.18166\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 19.2964 - decoder_loss: 19.1495 - encoder_loss: 0.0011 - classifier_loss: 0.1458 - decoder_accuracy: 0.0936 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1939 - val_decoder_loss: 19.4921 - val_encoder_loss: 1.0222 - val_classifier_loss: 0.6795 - val_decoder_accuracy: 0.0863 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.2567 - decoder_loss: 19.1124 - encoder_loss: 0.0014 - classifier_loss: 0.1428 - decoder_accuracy: 0.0968 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000\n",
            "Epoch 00037: val_loss did not improve from 21.18166\n",
            "2/2 [==============================] - 1s 308ms/step - loss: 19.2567 - decoder_loss: 19.1124 - encoder_loss: 0.0014 - classifier_loss: 0.1428 - decoder_accuracy: 0.0968 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000 - val_loss: 21.2172 - val_decoder_loss: 19.4857 - val_encoder_loss: 1.0495 - val_classifier_loss: 0.6819 - val_decoder_accuracy: 0.0884 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.1973 - decoder_loss: 19.0570 - encoder_loss: 5.9606e-04 - classifier_loss: 0.1397 - decoder_accuracy: 0.0967 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000\n",
            "Epoch 00038: val_loss improved from 21.18166 to 21.16019, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 19.1973 - decoder_loss: 19.0570 - encoder_loss: 5.9606e-04 - classifier_loss: 0.1397 - decoder_accuracy: 0.0967 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000 - val_loss: 21.1602 - val_decoder_loss: 19.4772 - val_encoder_loss: 1.0090 - val_classifier_loss: 0.6741 - val_decoder_accuracy: 0.0875 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.1344 - decoder_loss: 18.9957 - encoder_loss: 3.2876e-04 - classifier_loss: 0.1384 - decoder_accuracy: 0.0968 - encoder_accuracy: 0.0087 - classifier_accuracy: 1.0000\n",
            "Epoch 00039: val_loss improved from 21.16019 to 21.15987, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 19.1344 - decoder_loss: 18.9957 - encoder_loss: 3.2876e-04 - classifier_loss: 0.1384 - decoder_accuracy: 0.0968 - encoder_accuracy: 0.0087 - classifier_accuracy: 1.0000 - val_loss: 21.1599 - val_decoder_loss: 19.4647 - val_encoder_loss: 1.0188 - val_classifier_loss: 0.6764 - val_decoder_accuracy: 0.0817 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.1007 - decoder_loss: 18.9639 - encoder_loss: 9.0033e-04 - classifier_loss: 0.1358 - decoder_accuracy: 0.0961 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000\n",
            "Epoch 00040: val_loss did not improve from 21.15987\n",
            "2/2 [==============================] - 1s 231ms/step - loss: 19.1007 - decoder_loss: 18.9639 - encoder_loss: 9.0033e-04 - classifier_loss: 0.1358 - decoder_accuracy: 0.0961 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000 - val_loss: 21.1912 - val_decoder_loss: 19.4654 - val_encoder_loss: 1.0443 - val_classifier_loss: 0.6815 - val_decoder_accuracy: 0.0780 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.0681 - decoder_loss: 18.9327 - encoder_loss: 8.3019e-04 - classifier_loss: 0.1346 - decoder_accuracy: 0.0929 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000\n",
            "Epoch 00041: val_loss did not improve from 21.15987\n",
            "2/2 [==============================] - 1s 161ms/step - loss: 19.0681 - decoder_loss: 18.9327 - encoder_loss: 8.3019e-04 - classifier_loss: 0.1346 - decoder_accuracy: 0.0929 - encoder_accuracy: 0.0174 - classifier_accuracy: 1.0000 - val_loss: 21.1875 - val_decoder_loss: 19.4868 - val_encoder_loss: 1.0220 - val_classifier_loss: 0.6787 - val_decoder_accuracy: 0.0748 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5862 - lr: 0.0050\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 19.0276 - decoder_loss: 18.8913 - encoder_loss: 9.4991e-04 - classifier_loss: 0.1354 - decoder_accuracy: 0.0899 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00042: val_loss did not improve from 21.15987\n",
            "2/2 [==============================] - 1s 134ms/step - loss: 19.0276 - decoder_loss: 18.8913 - encoder_loss: 9.4991e-04 - classifier_loss: 0.1354 - decoder_accuracy: 0.0899 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1976 - val_decoder_loss: 19.4921 - val_encoder_loss: 1.0267 - val_classifier_loss: 0.6788 - val_decoder_accuracy: 0.0748 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.9762 - decoder_loss: 18.8419 - encoder_loss: 0.0013 - classifier_loss: 0.1330 - decoder_accuracy: 0.0897 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00043: val_loss did not improve from 21.15987\n",
            "2/2 [==============================] - 1s 150ms/step - loss: 18.9762 - decoder_loss: 18.8419 - encoder_loss: 0.0013 - classifier_loss: 0.1330 - decoder_accuracy: 0.0897 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1964 - val_decoder_loss: 19.4916 - val_encoder_loss: 1.0338 - val_classifier_loss: 0.6711 - val_decoder_accuracy: 0.0766 - val_encoder_accuracy: 0.0345 - val_classifier_accuracy: 0.5517 - lr: 0.0050\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.9298 - decoder_loss: 18.7972 - encoder_loss: 0.0012 - classifier_loss: 0.1314 - decoder_accuracy: 0.0896 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00044: val_loss did not improve from 21.15987\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "2/2 [==============================] - 1s 143ms/step - loss: 18.9298 - decoder_loss: 18.7972 - encoder_loss: 0.0012 - classifier_loss: 0.1314 - decoder_accuracy: 0.0896 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1676 - val_decoder_loss: 19.4853 - val_encoder_loss: 1.0126 - val_classifier_loss: 0.6697 - val_decoder_accuracy: 0.0761 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0050\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.8888 - decoder_loss: 18.7563 - encoder_loss: 0.0019 - classifier_loss: 0.1306 - decoder_accuracy: 0.0897 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00045: val_loss improved from 21.15987 to 21.14306, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 179ms/step - loss: 18.8888 - decoder_loss: 18.7563 - encoder_loss: 0.0019 - classifier_loss: 0.1306 - decoder_accuracy: 0.0897 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1431 - val_decoder_loss: 19.4727 - val_encoder_loss: 1.0016 - val_classifier_loss: 0.6687 - val_decoder_accuracy: 0.0756 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0025\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.8685 - decoder_loss: 18.7372 - encoder_loss: 6.4259e-04 - classifier_loss: 0.1306 - decoder_accuracy: 0.0897 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00046: val_loss did not improve from 21.14306\n",
            "2/2 [==============================] - 1s 156ms/step - loss: 18.8685 - decoder_loss: 18.7372 - encoder_loss: 6.4259e-04 - classifier_loss: 0.1306 - decoder_accuracy: 0.0897 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.2211 - val_decoder_loss: 19.4852 - val_encoder_loss: 1.0739 - val_classifier_loss: 0.6621 - val_decoder_accuracy: 0.0758 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5862 - lr: 0.0025\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.8422 - decoder_loss: 18.7131 - encoder_loss: 0.0010 - classifier_loss: 0.1280 - decoder_accuracy: 0.0896 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00047: val_loss improved from 21.14306 to 21.11334, saving model to logs/MIN2Net_out_weights.h5\n",
            "2/2 [==============================] - 1s 190ms/step - loss: 18.8422 - decoder_loss: 18.7131 - encoder_loss: 0.0010 - classifier_loss: 0.1280 - decoder_accuracy: 0.0896 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1133 - val_decoder_loss: 19.4505 - val_encoder_loss: 0.9896 - val_classifier_loss: 0.6732 - val_decoder_accuracy: 0.0762 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0025\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.8126 - decoder_loss: 18.6852 - encoder_loss: 5.8137e-04 - classifier_loss: 0.1268 - decoder_accuracy: 0.0906 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00048: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 144ms/step - loss: 18.8126 - decoder_loss: 18.6852 - encoder_loss: 5.8137e-04 - classifier_loss: 0.1268 - decoder_accuracy: 0.0906 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.2009 - val_decoder_loss: 19.4634 - val_encoder_loss: 1.0695 - val_classifier_loss: 0.6680 - val_decoder_accuracy: 0.0765 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.6207 - lr: 0.0025\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.7886 - decoder_loss: 18.6623 - encoder_loss: 0.0010 - classifier_loss: 0.1252 - decoder_accuracy: 0.0913 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00049: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 149ms/step - loss: 18.7886 - decoder_loss: 18.6623 - encoder_loss: 0.0010 - classifier_loss: 0.1252 - decoder_accuracy: 0.0913 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.2196 - val_decoder_loss: 19.4455 - val_encoder_loss: 1.1090 - val_classifier_loss: 0.6651 - val_decoder_accuracy: 0.0780 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0025\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.7672 - decoder_loss: 18.6426 - encoder_loss: 6.1742e-04 - classifier_loss: 0.1240 - decoder_accuracy: 0.0921 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00050: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 139ms/step - loss: 18.7672 - decoder_loss: 18.6426 - encoder_loss: 6.1742e-04 - classifier_loss: 0.1240 - decoder_accuracy: 0.0921 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1809 - val_decoder_loss: 19.4410 - val_encoder_loss: 1.0765 - val_classifier_loss: 0.6634 - val_decoder_accuracy: 0.0772 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0025\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.7458 - decoder_loss: 18.6225 - encoder_loss: 4.9348e-04 - classifier_loss: 0.1228 - decoder_accuracy: 0.0923 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00051: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 144ms/step - loss: 18.7458 - decoder_loss: 18.6225 - encoder_loss: 4.9348e-04 - classifier_loss: 0.1228 - decoder_accuracy: 0.0923 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1493 - val_decoder_loss: 19.4310 - val_encoder_loss: 1.0557 - val_classifier_loss: 0.6625 - val_decoder_accuracy: 0.0771 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0025\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.7236 - decoder_loss: 18.6007 - encoder_loss: 4.4344e-04 - classifier_loss: 0.1224 - decoder_accuracy: 0.0930 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00052: val_loss did not improve from 21.11334\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "2/2 [==============================] - 1s 155ms/step - loss: 18.7236 - decoder_loss: 18.6007 - encoder_loss: 4.4344e-04 - classifier_loss: 0.1224 - decoder_accuracy: 0.0930 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1961 - val_decoder_loss: 19.4297 - val_encoder_loss: 1.1086 - val_classifier_loss: 0.6577 - val_decoder_accuracy: 0.0781 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0025\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.7073 - decoder_loss: 18.5855 - encoder_loss: 7.2562e-04 - classifier_loss: 0.1211 - decoder_accuracy: 0.0941 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00053: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 149ms/step - loss: 18.7073 - decoder_loss: 18.5855 - encoder_loss: 7.2562e-04 - classifier_loss: 0.1211 - decoder_accuracy: 0.0941 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1552 - val_decoder_loss: 19.4289 - val_encoder_loss: 1.0646 - val_classifier_loss: 0.6617 - val_decoder_accuracy: 0.0774 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0012\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.7000 - decoder_loss: 18.5772 - encoder_loss: 7.7984e-04 - classifier_loss: 0.1220 - decoder_accuracy: 0.0939 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00054: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 152ms/step - loss: 18.7000 - decoder_loss: 18.5772 - encoder_loss: 7.7984e-04 - classifier_loss: 0.1220 - decoder_accuracy: 0.0939 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1743 - val_decoder_loss: 19.4253 - val_encoder_loss: 1.0706 - val_classifier_loss: 0.6784 - val_decoder_accuracy: 0.0771 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5862 - lr: 0.0012\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6851 - decoder_loss: 18.5641 - encoder_loss: 5.9646e-04 - classifier_loss: 0.1204 - decoder_accuracy: 0.0937 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00055: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 144ms/step - loss: 18.6851 - decoder_loss: 18.5641 - encoder_loss: 5.9646e-04 - classifier_loss: 0.1204 - decoder_accuracy: 0.0937 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1735 - val_decoder_loss: 19.4251 - val_encoder_loss: 1.0856 - val_classifier_loss: 0.6628 - val_decoder_accuracy: 0.0786 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0012\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6704 - decoder_loss: 18.5503 - encoder_loss: 5.4408e-04 - classifier_loss: 0.1196 - decoder_accuracy: 0.0942 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00056: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 144ms/step - loss: 18.6704 - decoder_loss: 18.5503 - encoder_loss: 5.4408e-04 - classifier_loss: 0.1196 - decoder_accuracy: 0.0942 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1801 - val_decoder_loss: 19.4213 - val_encoder_loss: 1.0942 - val_classifier_loss: 0.6646 - val_decoder_accuracy: 0.0783 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.6207 - lr: 0.0012\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6597 - decoder_loss: 18.5400 - encoder_loss: 4.7942e-04 - classifier_loss: 0.1193 - decoder_accuracy: 0.0940 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00057: val_loss did not improve from 21.11334\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "2/2 [==============================] - 1s 146ms/step - loss: 18.6597 - decoder_loss: 18.5400 - encoder_loss: 4.7942e-04 - classifier_loss: 0.1193 - decoder_accuracy: 0.0940 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.2334 - val_decoder_loss: 19.4213 - val_encoder_loss: 1.1509 - val_classifier_loss: 0.6612 - val_decoder_accuracy: 0.0766 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0012\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6489 - decoder_loss: 18.5298 - encoder_loss: 5.7476e-04 - classifier_loss: 0.1185 - decoder_accuracy: 0.0946 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00058: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 138ms/step - loss: 18.6489 - decoder_loss: 18.5298 - encoder_loss: 5.7476e-04 - classifier_loss: 0.1185 - decoder_accuracy: 0.0946 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1407 - val_decoder_loss: 19.4126 - val_encoder_loss: 1.0688 - val_classifier_loss: 0.6593 - val_decoder_accuracy: 0.0771 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6412 - decoder_loss: 18.5222 - encoder_loss: 4.7411e-04 - classifier_loss: 0.1185 - decoder_accuracy: 0.0945 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00059: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 141ms/step - loss: 18.6412 - decoder_loss: 18.5222 - encoder_loss: 4.7411e-04 - classifier_loss: 0.1185 - decoder_accuracy: 0.0945 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1700 - val_decoder_loss: 19.3990 - val_encoder_loss: 1.1164 - val_classifier_loss: 0.6546 - val_decoder_accuracy: 0.0769 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6309 - decoder_loss: 18.5144 - encoder_loss: 5.4556e-04 - classifier_loss: 0.1160 - decoder_accuracy: 0.0946 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00060: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 137ms/step - loss: 18.6309 - decoder_loss: 18.5144 - encoder_loss: 5.4556e-04 - classifier_loss: 0.1160 - decoder_accuracy: 0.0946 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1519 - val_decoder_loss: 19.4099 - val_encoder_loss: 1.0852 - val_classifier_loss: 0.6568 - val_decoder_accuracy: 0.0768 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5862 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6219 - decoder_loss: 18.5057 - encoder_loss: 1.7289e-04 - classifier_loss: 0.1160 - decoder_accuracy: 0.0946 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00061: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 145ms/step - loss: 18.6219 - decoder_loss: 18.5057 - encoder_loss: 1.7289e-04 - classifier_loss: 0.1160 - decoder_accuracy: 0.0946 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1552 - val_decoder_loss: 19.3966 - val_encoder_loss: 1.0987 - val_classifier_loss: 0.6600 - val_decoder_accuracy: 0.0777 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5517 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6131 - decoder_loss: 18.4973 - encoder_loss: 2.2533e-04 - classifier_loss: 0.1157 - decoder_accuracy: 0.0941 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00062: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 147ms/step - loss: 18.6131 - decoder_loss: 18.4973 - encoder_loss: 2.2533e-04 - classifier_loss: 0.1157 - decoder_accuracy: 0.0941 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1684 - val_decoder_loss: 19.3960 - val_encoder_loss: 1.1153 - val_classifier_loss: 0.6571 - val_decoder_accuracy: 0.0774 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5862 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.6049 - decoder_loss: 18.4886 - encoder_loss: 6.0594e-05 - classifier_loss: 0.1163 - decoder_accuracy: 0.0934 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000\n",
            "Epoch 00063: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 138ms/step - loss: 18.6049 - decoder_loss: 18.4886 - encoder_loss: 6.0594e-05 - classifier_loss: 0.1163 - decoder_accuracy: 0.0934 - encoder_accuracy: 0.0261 - classifier_accuracy: 1.0000 - val_loss: 21.1875 - val_decoder_loss: 19.4113 - val_encoder_loss: 1.1180 - val_classifier_loss: 0.6583 - val_decoder_accuracy: 0.0771 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4828 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.5964 - decoder_loss: 18.4805 - encoder_loss: 2.6647e-04 - classifier_loss: 0.1156 - decoder_accuracy: 0.0933 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00064: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 141ms/step - loss: 18.5964 - decoder_loss: 18.4805 - encoder_loss: 2.6647e-04 - classifier_loss: 0.1156 - decoder_accuracy: 0.0933 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1604 - val_decoder_loss: 19.3923 - val_encoder_loss: 1.1116 - val_classifier_loss: 0.6565 - val_decoder_accuracy: 0.0778 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.4828 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.5885 - decoder_loss: 18.4733 - encoder_loss: 2.2935e-04 - classifier_loss: 0.1151 - decoder_accuracy: 0.0923 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00065: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 150ms/step - loss: 18.5885 - decoder_loss: 18.4733 - encoder_loss: 2.2935e-04 - classifier_loss: 0.1151 - decoder_accuracy: 0.0923 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.2072 - val_decoder_loss: 19.4087 - val_encoder_loss: 1.1286 - val_classifier_loss: 0.6699 - val_decoder_accuracy: 0.0771 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.6207 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.5784 - decoder_loss: 18.4648 - encoder_loss: 3.4877e-04 - classifier_loss: 0.1133 - decoder_accuracy: 0.0926 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00066: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 143ms/step - loss: 18.5784 - decoder_loss: 18.4648 - encoder_loss: 3.4877e-04 - classifier_loss: 0.1133 - decoder_accuracy: 0.0926 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1650 - val_decoder_loss: 19.4040 - val_encoder_loss: 1.1095 - val_classifier_loss: 0.6515 - val_decoder_accuracy: 0.0759 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5862 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 18.5710 - decoder_loss: 18.4578 - encoder_loss: 3.3897e-04 - classifier_loss: 0.1129 - decoder_accuracy: 0.0927 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000\n",
            "Epoch 00067: val_loss did not improve from 21.11334\n",
            "2/2 [==============================] - 1s 154ms/step - loss: 18.5710 - decoder_loss: 18.4578 - encoder_loss: 3.3897e-04 - classifier_loss: 0.1129 - decoder_accuracy: 0.0927 - encoder_accuracy: 0.0348 - classifier_accuracy: 1.0000 - val_loss: 21.1216 - val_decoder_loss: 19.3880 - val_encoder_loss: 1.0768 - val_classifier_loss: 0.6569 - val_decoder_accuracy: 0.0769 - val_encoder_accuracy: 0.0000e+00 - val_classifier_accuracy: 0.5172 - lr: 0.0010\n",
            "Epoch 00067: early stopping\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1, 400, 20)]      0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 400, 20)        25620     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1, 400, 20)       80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  (None, 1, 100, 20)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 1, 100, 10)        6410      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1, 100, 10)       40        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " average_pooling2d_3 (Averag  (None, 1, 25, 10)        0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 250)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                5020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,170\n",
            "Trainable params: 37,110\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_input (InputLayer)  [(None, 20)]              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 250)               5250      \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 1, 25, 10)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 1, 100, 10)       6410      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 1, 400, 20)       6420      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,080\n",
            "Trainable params: 18,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"MIN2Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 1, 400, 20)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, 20)           37170       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, 1, 400, 20)   18080       ['encoder[0][0]']                \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 2)            42          ['encoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55,292\n",
            "Trainable params: 55,232\n",
            "Non-trainable params: 60\n",
            "__________________________________________________________________________________________________\n",
            "2/2 [==============================] - 1s 76ms/step - loss: 21.9375 - decoder_loss: 20.4948 - encoder_loss: 0.8103 - classifier_loss: 0.6324 - decoder_accuracy: 0.0789 - encoder_accuracy: 0.0417 - classifier_accuracy: 0.6667\n",
            "F1-score is computed based on binary\n",
            "(loss: 21.93746566772461, accuracy: 0.6666666865348816)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.68      0.67        72\n",
            "         1.0       0.67      0.65      0.66        72\n",
            "\n",
            "    accuracy                           0.67       144\n",
            "   macro avg       0.67      0.67      0.67       144\n",
            "weighted avg       0.67      0.67      0.67       144\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EEGNet model example\n",
        "https://iopscience.iop.org/article/10.1088/1741-2552/aace8c"
      ],
      "metadata": {
        "id": "WZkzU_xXVcrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(dataset='BCIC2a', \n",
        "                    train_type='subject_dependent', \n",
        "                    subject=1, \n",
        "                    data_format='NCTD', # for EEGNet and DeepConvNet (our paper and the original paper set data_format='NDCT')\n",
        "                    data_type='time_domain', \n",
        "                    dataset_path='datasets')"
      ],
      "metadata": {
        "id": "QomyxxP-YKd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "X_train, y_train = loader.load_train_set(fold=1)\n",
        "X_val, y_val = loader.load_val_set(fold=1)\n",
        "X_test, y_test = loader.load_test_set(fold=1)\n",
        "\n",
        "from min2net.model import EEGNet\n",
        "# (our paper and the original paper set input_shape=(1,20,400), data_format='channels_first')\n",
        "model = EEGNet(input_shape=(20,400,1), num_class=2, dropout_rate=0.25, shuffle=True, data_format='channels_last')\n",
        "model.fit(X_train, y_train, X_val, y_val)\n",
        "Y, evaluation = model.predict(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxNiR3fpTzlU",
        "outputId": "7f2cb8b1-b96c-4958-ebcb-98a52598df60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change data_format to 'NCTD', new dimention is (115, 20, 400, 1)\n",
            "change data_format to 'NCTD', new dimention is (29, 20, 400, 1)\n",
            "change data_format to 'NCTD', new dimention is (144, 20, 400, 1)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20, 400, 1)]      0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 20, 400, 8)        1600      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 20, 400, 8)       32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " depthwise_conv2d (Depthwise  (None, 1, 400, 16)       320       \n",
            " Conv2D)                                                         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1, 400, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1, 400, 16)        0         \n",
            "                                                                 \n",
            " average_pooling2d_4 (Averag  (None, 1, 100, 16)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1, 100, 16)        0         \n",
            "                                                                 \n",
            " separable_conv2d (Separable  (None, 1, 100, 16)       1056      \n",
            " Conv2D)                                                         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 1, 100, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1, 100, 16)        0         \n",
            "                                                                 \n",
            " average_pooling2d_5 (Averag  (None, 1, 12, 16)        0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1, 12, 16)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 192)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 386       \n",
            "                                                                 \n",
            " softmax (Activation)        (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,522\n",
            "Trainable params: 3,442\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "The first kernel size is (1, 200)\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7303 - accuracy: 0.4348\n",
            "Epoch 00001: val_loss improved from inf to 0.68895, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 3s 681ms/step - loss: 0.7303 - accuracy: 0.4348 - val_loss: 0.6890 - val_accuracy: 0.6897 - lr: 0.0100\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.5043\n",
            "Epoch 00002: val_loss improved from 0.68895 to 0.67428, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 504ms/step - loss: 0.6814 - accuracy: 0.5043 - val_loss: 0.6743 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6329 - accuracy: 0.6522\n",
            "Epoch 00003: val_loss did not improve from 0.67428\n",
            "2/2 [==============================] - 2s 457ms/step - loss: 0.6329 - accuracy: 0.6522 - val_loss: 0.6855 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.7217\n",
            "Epoch 00004: val_loss improved from 0.67428 to 0.65638, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 504ms/step - loss: 0.5595 - accuracy: 0.7217 - val_loss: 0.6564 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.7739\n",
            "Epoch 00005: val_loss improved from 0.65638 to 0.61702, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 514ms/step - loss: 0.4931 - accuracy: 0.7739 - val_loss: 0.6170 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.7652\n",
            "Epoch 00006: val_loss improved from 0.61702 to 0.56278, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 512ms/step - loss: 0.4618 - accuracy: 0.7652 - val_loss: 0.5628 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4037 - accuracy: 0.8261\n",
            "Epoch 00007: val_loss improved from 0.56278 to 0.52122, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 506ms/step - loss: 0.4037 - accuracy: 0.8261 - val_loss: 0.5212 - val_accuracy: 0.6897 - lr: 0.0100\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8609\n",
            "Epoch 00008: val_loss improved from 0.52122 to 0.50372, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 502ms/step - loss: 0.3929 - accuracy: 0.8609 - val_loss: 0.5037 - val_accuracy: 0.7241 - lr: 0.0100\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.8522\n",
            "Epoch 00009: val_loss did not improve from 0.50372\n",
            "2/2 [==============================] - 2s 458ms/step - loss: 0.3758 - accuracy: 0.8522 - val_loss: 0.5239 - val_accuracy: 0.7931 - lr: 0.0100\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8609\n",
            "Epoch 00010: val_loss did not improve from 0.50372\n",
            "2/2 [==============================] - 2s 464ms/step - loss: 0.3423 - accuracy: 0.8609 - val_loss: 0.5129 - val_accuracy: 0.7931 - lr: 0.0100\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.9043\n",
            "Epoch 00011: val_loss improved from 0.50372 to 0.50286, saving model to logs/EEGNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 502ms/step - loss: 0.3093 - accuracy: 0.9043 - val_loss: 0.5029 - val_accuracy: 0.7931 - lr: 0.0100\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3210 - accuracy: 0.8870\n",
            "Epoch 00012: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 484ms/step - loss: 0.3210 - accuracy: 0.8870 - val_loss: 0.5186 - val_accuracy: 0.7931 - lr: 0.0100\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8522\n",
            "Epoch 00013: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 467ms/step - loss: 0.3305 - accuracy: 0.8522 - val_loss: 0.5337 - val_accuracy: 0.8276 - lr: 0.0100\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9043\n",
            "Epoch 00014: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 469ms/step - loss: 0.2640 - accuracy: 0.9043 - val_loss: 0.6060 - val_accuracy: 0.7931 - lr: 0.0100\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8957\n",
            "Epoch 00015: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 474ms/step - loss: 0.2856 - accuracy: 0.8957 - val_loss: 0.8205 - val_accuracy: 0.6897 - lr: 0.0100\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.8783\n",
            "Epoch 00016: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 460ms/step - loss: 0.2561 - accuracy: 0.8783 - val_loss: 0.8910 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.8957\n",
            "Epoch 00017: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 465ms/step - loss: 0.2743 - accuracy: 0.8957 - val_loss: 0.8851 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.9130\n",
            "Epoch 00018: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 468ms/step - loss: 0.2694 - accuracy: 0.9130 - val_loss: 0.9537 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.8870\n",
            "Epoch 00019: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 3s 592ms/step - loss: 0.2506 - accuracy: 0.8870 - val_loss: 1.0705 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2191 - accuracy: 0.9130\n",
            "Epoch 00020: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 461ms/step - loss: 0.2191 - accuracy: 0.9130 - val_loss: 0.9755 - val_accuracy: 0.6897 - lr: 0.0100\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2268 - accuracy: 0.9043\n",
            "Epoch 00021: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 461ms/step - loss: 0.2268 - accuracy: 0.9043 - val_loss: 0.8304 - val_accuracy: 0.7586 - lr: 0.0100\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9391\n",
            "Epoch 00022: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 457ms/step - loss: 0.2072 - accuracy: 0.9391 - val_loss: 0.9522 - val_accuracy: 0.7586 - lr: 0.0100\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9043\n",
            "Epoch 00023: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 469ms/step - loss: 0.2323 - accuracy: 0.9043 - val_loss: 1.1423 - val_accuracy: 0.6897 - lr: 0.0100\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9304\n",
            "Epoch 00024: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 469ms/step - loss: 0.2064 - accuracy: 0.9304 - val_loss: 1.1839 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9304\n",
            "Epoch 00025: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 467ms/step - loss: 0.2193 - accuracy: 0.9304 - val_loss: 1.0353 - val_accuracy: 0.6897 - lr: 0.0100\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9043\n",
            "Epoch 00026: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 3s 769ms/step - loss: 0.2022 - accuracy: 0.9043 - val_loss: 0.7841 - val_accuracy: 0.7931 - lr: 0.0100\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9478\n",
            "Epoch 00027: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 3s 472ms/step - loss: 0.1773 - accuracy: 0.9478 - val_loss: 0.7214 - val_accuracy: 0.7241 - lr: 0.0100\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9304\n",
            "Epoch 00028: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 460ms/step - loss: 0.2013 - accuracy: 0.9304 - val_loss: 0.7568 - val_accuracy: 0.7241 - lr: 0.0100\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9565\n",
            "Epoch 00029: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 454ms/step - loss: 0.1483 - accuracy: 0.9565 - val_loss: 1.1415 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9565\n",
            "Epoch 00030: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 462ms/step - loss: 0.1405 - accuracy: 0.9565 - val_loss: 1.6245 - val_accuracy: 0.6552 - lr: 0.0100\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1617 - accuracy: 0.9478\n",
            "Epoch 00031: val_loss did not improve from 0.50286\n",
            "2/2 [==============================] - 2s 464ms/step - loss: 0.1617 - accuracy: 0.9478 - val_loss: 1.8065 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 00031: early stopping\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.4269 - accuracy: 0.7986\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.62      0.76        72\n",
            "         1.0       0.72      0.97      0.83        72\n",
            "\n",
            "    accuracy                           0.80       144\n",
            "   macro avg       0.84      0.80      0.79       144\n",
            "weighted avg       0.84      0.80      0.79       144\n",
            "\n",
            "F1-score is computed based on binary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepConvNet model example\n",
        "https://doi.org/10.1002/hbm.2373"
      ],
      "metadata": {
        "id": "MZvBkVMEVjGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "X_train, y_train = loader.load_train_set(fold=1)\n",
        "X_val, y_val = loader.load_val_set(fold=1)\n",
        "X_test, y_test = loader.load_test_set(fold=1)\n",
        "\n",
        "from min2net.model import DeepConvNet\n",
        "# (our paper and the original paper set input_shape=(1,20,400), data_format='channels_first')\n",
        "model = DeepConvNet(input_shape=(20,400,1), num_class=2, dropout_rate=0.25, shuffle=True, data_format='channels_last')\n",
        "model.fit(X_train, y_train, X_val, y_val)\n",
        "Y, evaluation = model.predict(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUs2K937Ulzc",
        "outputId": "95862545-1012-4edf-ca8f-09d4a38162bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change data_format to 'NCTD', new dimention is (115, 20, 400, 1)\n",
            "change data_format to 'NCTD', new dimention is (29, 20, 400, 1)\n",
            "change data_format to 'NCTD', new dimention is (144, 20, 400, 1)\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 20, 400, 1)]      0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 20, 396, 25)       150       \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 1, 396, 25)        12525     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 1, 396, 25)       4         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 1, 396, 25)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 1, 198, 25)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1, 198, 25)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 1, 194, 50)        6300      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1, 194, 50)       4         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1, 194, 50)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 1, 97, 50)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1, 97, 50)         0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 1, 93, 100)        25100     \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 1, 93, 100)       4         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 1, 93, 100)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 46, 100)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1, 46, 100)        0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 1, 42, 200)        100200    \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 1, 42, 200)       4         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 1, 42, 200)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 21, 200)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1, 21, 200)        0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4200)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 8402      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 152,693\n",
            "Trainable params: 152,685\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.2422 - accuracy: 0.6000\n",
            "Epoch 00001: val_loss improved from inf to 31.68350, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 2s 480ms/step - loss: 1.2422 - accuracy: 0.6000 - val_loss: 31.6835 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 12.0616 - accuracy: 0.4870\n",
            "Epoch 00002: val_loss improved from 31.68350 to 10.86022, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 12.0616 - accuracy: 0.4870 - val_loss: 10.8602 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 3.4153 - accuracy: 0.4870\n",
            "Epoch 00003: val_loss improved from 10.86022 to 1.29985, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 3.4153 - accuracy: 0.4870 - val_loss: 1.2998 - val_accuracy: 0.3793 - lr: 0.0100\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.7523 - accuracy: 0.4783\n",
            "Epoch 00004: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 204ms/step - loss: 1.7523 - accuracy: 0.4783 - val_loss: 8.4415 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 3.5286 - accuracy: 0.4957\n",
            "Epoch 00005: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 3.5286 - accuracy: 0.4957 - val_loss: 3.6614 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 3.2718 - accuracy: 0.5130\n",
            "Epoch 00006: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 211ms/step - loss: 3.2718 - accuracy: 0.5130 - val_loss: 2.3753 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2.0543 - accuracy: 0.5478\n",
            "Epoch 00007: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 204ms/step - loss: 2.0543 - accuracy: 0.5478 - val_loss: 3.5733 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2.2155 - accuracy: 0.5130\n",
            "Epoch 00008: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 205ms/step - loss: 2.2155 - accuracy: 0.5130 - val_loss: 3.7187 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 2.2993 - accuracy: 0.5217\n",
            "Epoch 00009: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 207ms/step - loss: 2.2993 - accuracy: 0.5217 - val_loss: 1.9080 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.0638 - accuracy: 0.5478\n",
            "Epoch 00010: val_loss did not improve from 1.29985\n",
            "2/2 [==============================] - 1s 195ms/step - loss: 1.0638 - accuracy: 0.5478 - val_loss: 1.8259 - val_accuracy: 0.3448 - lr: 0.0100\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.4655 - accuracy: 0.5391\n",
            "Epoch 00011: val_loss improved from 1.29985 to 1.25236, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 1.4655 - accuracy: 0.5391 - val_loss: 1.2524 - val_accuracy: 0.3793 - lr: 0.0100\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9009 - accuracy: 0.5826\n",
            "Epoch 00012: val_loss did not improve from 1.25236\n",
            "2/2 [==============================] - 1s 207ms/step - loss: 0.9009 - accuracy: 0.5826 - val_loss: 1.2794 - val_accuracy: 0.4483 - lr: 0.0100\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.1431 - accuracy: 0.5565\n",
            "Epoch 00013: val_loss improved from 1.25236 to 1.20405, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 1.1431 - accuracy: 0.5565 - val_loss: 1.2041 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.2131 - accuracy: 0.5652\n",
            "Epoch 00014: val_loss improved from 1.20405 to 0.83519, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 1.2131 - accuracy: 0.5652 - val_loss: 0.8352 - val_accuracy: 0.5517 - lr: 0.0100\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.5391\n",
            "Epoch 00015: val_loss improved from 0.83519 to 0.81953, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.7603 - accuracy: 0.5391 - val_loss: 0.8195 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.6174\n",
            "Epoch 00016: val_loss did not improve from 0.81953\n",
            "2/2 [==============================] - 1s 195ms/step - loss: 0.7661 - accuracy: 0.6174 - val_loss: 0.8220 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.5739\n",
            "Epoch 00017: val_loss improved from 0.81953 to 0.73804, saving model to logs/DeepConvNet_out_weights.h5\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.9186 - accuracy: 0.5739 - val_loss: 0.7380 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.6087\n",
            "Epoch 00018: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 0.7115 - accuracy: 0.6087 - val_loss: 0.7552 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.6087\n",
            "Epoch 00019: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 0.6551 - accuracy: 0.6087 - val_loss: 0.9859 - val_accuracy: 0.5517 - lr: 0.0100\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7281 - accuracy: 0.6261\n",
            "Epoch 00020: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 0.7281 - accuracy: 0.6261 - val_loss: 0.9498 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7249 - accuracy: 0.6174\n",
            "Epoch 00021: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 207ms/step - loss: 0.7249 - accuracy: 0.6174 - val_loss: 0.8060 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7145 - accuracy: 0.6348\n",
            "Epoch 00022: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.7145 - accuracy: 0.6348 - val_loss: 0.9049 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7576 - accuracy: 0.6522\n",
            "Epoch 00023: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 195ms/step - loss: 0.7576 - accuracy: 0.6522 - val_loss: 0.8506 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.6174\n",
            "Epoch 00024: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 193ms/step - loss: 0.6897 - accuracy: 0.6174 - val_loss: 0.9194 - val_accuracy: 0.5517 - lr: 0.0100\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.6435\n",
            "Epoch 00025: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.6676 - accuracy: 0.6435 - val_loss: 0.9114 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7597 - accuracy: 0.6000\n",
            "Epoch 00026: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.7597 - accuracy: 0.6000 - val_loss: 0.9166 - val_accuracy: 0.4483 - lr: 0.0100\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.6174\n",
            "Epoch 00027: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 210ms/step - loss: 0.6822 - accuracy: 0.6174 - val_loss: 0.7558 - val_accuracy: 0.4483 - lr: 0.0100\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.5913\n",
            "Epoch 00028: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 208ms/step - loss: 0.6647 - accuracy: 0.5913 - val_loss: 0.8267 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.6174\n",
            "Epoch 00029: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.6656 - accuracy: 0.6174 - val_loss: 0.8237 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.6783\n",
            "Epoch 00030: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.5731 - accuracy: 0.6783 - val_loss: 0.8586 - val_accuracy: 0.5862 - lr: 0.0100\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.7130\n",
            "Epoch 00031: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 211ms/step - loss: 0.5843 - accuracy: 0.7130 - val_loss: 0.8425 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.6522\n",
            "Epoch 00032: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 196ms/step - loss: 0.5851 - accuracy: 0.6522 - val_loss: 0.9386 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.6783\n",
            "Epoch 00033: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.5915 - accuracy: 0.6783 - val_loss: 0.8249 - val_accuracy: 0.4483 - lr: 0.0100\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.6783\n",
            "Epoch 00034: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 209ms/step - loss: 0.6061 - accuracy: 0.6783 - val_loss: 0.9565 - val_accuracy: 0.4138 - lr: 0.0100\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.7130\n",
            "Epoch 00035: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 0.5665 - accuracy: 0.7130 - val_loss: 0.8868 - val_accuracy: 0.5172 - lr: 0.0100\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.6957\n",
            "Epoch 00036: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 204ms/step - loss: 0.5468 - accuracy: 0.6957 - val_loss: 0.8652 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.6957\n",
            "Epoch 00037: val_loss did not improve from 0.73804\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.5531 - accuracy: 0.6957 - val_loss: 0.8079 - val_accuracy: 0.4828 - lr: 0.0100\n",
            "Epoch 00037: early stopping\n",
            "2/2 [==============================] - 1s 99ms/step - loss: 1.0387 - accuracy: 0.4514\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.46      0.53      0.49        72\n",
            "         1.0       0.44      0.38      0.41        72\n",
            "\n",
            "    accuracy                           0.45       144\n",
            "   macro avg       0.45      0.45      0.45       144\n",
            "weighted avg       0.45      0.45      0.45       144\n",
            "\n",
            "F1-score is computed based on binary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SpectralSpatialCNN model example\n",
        "https://doi.org/10.1109/TNNLS.2019.2946869"
      ],
      "metadata": {
        "id": "Hfz9jRGjVmmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this model requires spectral-spatial-mapping\n",
        "# see https://min2net.github.io/docs/preprocessing/BCIC2a/#spectral-spatial-mapping\n",
        "\n",
        "# generate fake data\n",
        "import numpy as np\n",
        "X_train = np.random.rand(100,20,28,28,1)\n",
        "y_train = np.concatenate(([0]*50, [1]*50))\n",
        "X_val = np.random.rand(40,20,28,28,1)\n",
        "y_val = np.concatenate(([0]*20, [1]*20))\n",
        "X_test = np.random.rand(40,20,28,28,1)\n",
        "y_test = np.concatenate(([0]*20, [1]*20))\n",
        "\n",
        "from min2net.model import SpectralSpatialCNN\n",
        "model = SpectralSpatialCNN(input_shape=(28, 28, 1), num_class=2, epochs=10, dropout_rate=0.25, shuffle=True)\n",
        "model.fit(X_train, y_train, X_val, y_val)\n",
        "Y, evaluation = model.predict(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np-wRA1tVG9W",
        "outputId": "1ec700c0-baff-48a1-fabf-919ff9701e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_15 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_17 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_19 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_21 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_25 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 28, 28, 10)   100         ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 28, 28, 10)   100         ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 28, 28, 10)   100         ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 28, 28, 10)   100         ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 28, 28, 10)   100         ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 28, 28, 10)   100         ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 28, 28, 10)   100         ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 28, 28, 10)   100         ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 28, 28, 10)   100         ['input_15[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 28, 28, 10)   100         ['input_16[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 28, 28, 10)   100         ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 28, 28, 10)   100         ['input_18[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 28, 28, 10)   100         ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 28, 28, 10)   100         ['input_20[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 28, 28, 10)   100         ['input_21[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 28, 28, 10)   100         ['input_22[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 28, 28, 10)   100         ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 28, 28, 10)   100         ['input_24[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 28, 28, 10)   100         ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 28, 28, 10)   100         ['input_26[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_52[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_55[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_64[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_67[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_70[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 28, 28, 14)   1274        ['conv2d_73[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_44[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_56[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_59[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_65[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_68[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_71[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 28, 28, 18)   2286        ['conv2d_74[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 14112)        0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 14112)        0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)            (None, 14112)        0           ['conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_7 (Flatten)            (None, 14112)        0           ['conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_8 (Flatten)            (None, 14112)        0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_9 (Flatten)            (None, 14112)        0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_10 (Flatten)           (None, 14112)        0           ['conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_11 (Flatten)           (None, 14112)        0           ['conv2d_39[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_12 (Flatten)           (None, 14112)        0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_13 (Flatten)           (None, 14112)        0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_14 (Flatten)           (None, 14112)        0           ['conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_15 (Flatten)           (None, 14112)        0           ['conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_16 (Flatten)           (None, 14112)        0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_17 (Flatten)           (None, 14112)        0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_18 (Flatten)           (None, 14112)        0           ['conv2d_60[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_19 (Flatten)           (None, 14112)        0           ['conv2d_63[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_20 (Flatten)           (None, 14112)        0           ['conv2d_66[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_21 (Flatten)           (None, 14112)        0           ['conv2d_69[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_22 (Flatten)           (None, 14112)        0           ['conv2d_72[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_23 (Flatten)           (None, 14112)        0           ['conv2d_75[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 256)          3612928     ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 256)          3612928     ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 256)          3612928     ['flatten_6[0][0]']              \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 256)          3612928     ['flatten_7[0][0]']              \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 256)          3612928     ['flatten_8[0][0]']              \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 256)          3612928     ['flatten_9[0][0]']              \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 256)          3612928     ['flatten_10[0][0]']             \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 256)          3612928     ['flatten_11[0][0]']             \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 256)          3612928     ['flatten_12[0][0]']             \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 256)          3612928     ['flatten_13[0][0]']             \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 256)          3612928     ['flatten_14[0][0]']             \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 256)          3612928     ['flatten_15[0][0]']             \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 256)          3612928     ['flatten_16[0][0]']             \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 256)          3612928     ['flatten_17[0][0]']             \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 256)          3612928     ['flatten_18[0][0]']             \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 256)          3612928     ['flatten_19[0][0]']             \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 256)          3612928     ['flatten_20[0][0]']             \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 256)          3612928     ['flatten_21[0][0]']             \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 256)          3612928     ['flatten_22[0][0]']             \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 256)          3612928     ['flatten_23[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5120)         0           ['dense_6[0][0]',                \n",
            "                                                                  'dense_7[0][0]',                \n",
            "                                                                  'dense_8[0][0]',                \n",
            "                                                                  'dense_9[0][0]',                \n",
            "                                                                  'dense_10[0][0]',               \n",
            "                                                                  'dense_11[0][0]',               \n",
            "                                                                  'dense_12[0][0]',               \n",
            "                                                                  'dense_13[0][0]',               \n",
            "                                                                  'dense_14[0][0]',               \n",
            "                                                                  'dense_15[0][0]',               \n",
            "                                                                  'dense_16[0][0]',               \n",
            "                                                                  'dense_17[0][0]',               \n",
            "                                                                  'dense_18[0][0]',               \n",
            "                                                                  'dense_19[0][0]',               \n",
            "                                                                  'dense_20[0][0]',               \n",
            "                                                                  'dense_21[0][0]',               \n",
            "                                                                  'dense_22[0][0]',               \n",
            "                                                                  'dense_23[0][0]',               \n",
            "                                                                  'dense_24[0][0]',               \n",
            "                                                                  'dense_25[0][0]']               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 1024)         5243904     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 1024)         0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 2)            2050        ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 77,577,714\n",
            "Trainable params: 77,577,714\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7022 - accuracy: 0.4500\n",
            "Epoch 00001: val_loss improved from inf to 0.68741, saving model to logs/SpectralSpatialCNN_out_weights.h5\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.7022 - accuracy: 0.4500 - val_loss: 0.6874 - val_accuracy: 0.4500 - lr: 1.0000e-05\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.5400\n",
            "Epoch 00002: val_loss improved from 0.68741 to 0.68565, saving model to logs/SpectralSpatialCNN_out_weights.h5\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.6831 - accuracy: 0.5400 - val_loss: 0.6856 - val_accuracy: 0.5500 - lr: 1.0000e-05\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6690 - accuracy: 0.6300\n",
            "Epoch 00003: val_loss did not improve from 0.68565\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.6690 - accuracy: 0.6300 - val_loss: 0.6874 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.6900\n",
            "Epoch 00004: val_loss did not improve from 0.68565\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6555 - accuracy: 0.6900 - val_loss: 0.6866 - val_accuracy: 0.6250 - lr: 1.0000e-05\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6429 - accuracy: 0.7300\n",
            "Epoch 00005: val_loss improved from 0.68565 to 0.68525, saving model to logs/SpectralSpatialCNN_out_weights.h5\n",
            "1/1 [==============================] - 17s 17s/step - loss: 0.6429 - accuracy: 0.7300 - val_loss: 0.6852 - val_accuracy: 0.5250 - lr: 1.0000e-05\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6339 - accuracy: 0.8300\n",
            "Epoch 00006: val_loss did not improve from 0.68525\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6339 - accuracy: 0.8300 - val_loss: 0.6861 - val_accuracy: 0.5250 - lr: 1.0000e-05\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6278 - accuracy: 0.7300\n",
            "Epoch 00007: val_loss did not improve from 0.68525\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6278 - accuracy: 0.7300 - val_loss: 0.6858 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6087 - accuracy: 0.8400\n",
            "Epoch 00008: val_loss improved from 0.68525 to 0.68500, saving model to logs/SpectralSpatialCNN_out_weights.h5\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.6087 - accuracy: 0.8400 - val_loss: 0.6850 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.9600\n",
            "Epoch 00009: val_loss did not improve from 0.68500\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5965 - accuracy: 0.9600 - val_loss: 0.6853 - val_accuracy: 0.5750 - lr: 1.0000e-05\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5769 - accuracy: 0.9400\n",
            "Epoch 00010: val_loss did not improve from 0.68500\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.5769 - accuracy: 0.9400 - val_loss: 0.6857 - val_accuracy: 0.6250 - lr: 1.0000e-05\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7072 - accuracy: 0.4000\n",
            "F1-score is comptured basen on binary\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.60      0.50        20\n",
            "           1       0.33      0.20      0.25        20\n",
            "\n",
            "    accuracy                           0.40        40\n",
            "   macro avg       0.38      0.40      0.38        40\n",
            "weighted avg       0.38      0.40      0.38        40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JcKFlcGmaSWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}